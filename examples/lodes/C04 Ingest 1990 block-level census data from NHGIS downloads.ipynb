{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, json, glob, math, numpy, os, pandas, re, scipy, scipy.sparse, shutil\n",
    "import subprocess, sys, threading, time, urllib2\n",
    "\n",
    "def exec_ipynb(filename_or_url):\n",
    "    nb = (urllib2.urlopen(filename_or_url) if re.match(r'https?:', filename_or_url) else open(filename_or_url)).read()\n",
    "    jsonNb = json.loads(nb)\n",
    "    #check for the modified formatting of Jupyter Notebook v4\n",
    "    if(jsonNb['nbformat'] == 4):\n",
    "        exec '\\n'.join([''.join(cell['source']) for cell in jsonNb['cells'] if cell['cell_type'] == 'code']) in globals()\n",
    "    else:\n",
    "        exec '\\n'.join([''.join(cell['input']) for cell in jsonNb['worksheets'][0]['cells'] if cell['cell_type'] == 'code']) in globals()\n",
    "\n",
    "exec_ipynb('timelapse-utilities.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs files from NHGIS\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "capture90 = 'capture/NHGIS_1990'\n",
    "crosswalk_file = 'capture/NHGIS_1990/NHGIS_block1990_to_block2010/crosswalk_block1990_block2010_v001.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NHGIS_block1990_to_block2010.zip\r",
      "\r\n",
      "*PRELIMARY* crosswalk between 1990 and 2010 census blocks\r",
      "\r\n",
      "\r",
      "\r\n",
      "Producer: National Historical Geographic Information System (NHGIS)\r",
      "\r\n",
      "NHGIS citation: See https://nhgis.org/research/citation for current citation\r",
      "\r\n",
      "\r",
      "\r\n",
      "___Summary___\r",
      "\r\n",
      "File name: crosswalk_block1990_block2010_v001.csv\r",
      "\r\n",
      "Content:\r",
      "\r\n",
      "\t- Each row represents an intersection between a 1990 block and 2010 block\r",
      "\r\n",
      "\t- The FID1990 and FID2010 fields contain block IDs preceded arbitrarily by an \"F\" to prevent the loss of leading zeros.\r",
      "\r\n",
      "\t\t- The FID1990 field contains numerous values of \"NULL\". These represent cases where the only 1990 blocks intersecting the corresponding 2010 block are offshore, lying in coastal or Great Lakes waters, which are excluded from NHGIS's block boundary files. None of the missing 1990 blocks had any reported population or housing units. The NULL records are included here to ensure that all 2010 blocks are represented in the file.\r",
      "\r\n",
      "\t- The WEIGHT field contains the interpolation weights NHGIS will use to allocate portions of 1990 block counts to 2010 blocks\r",
      "\r\n",
      "\t- The PAREA_VIA_B2000 field contains the approximate portion of the 1990 block's land area lying in the 2010 block, based on intersections the 1990 and 2010 block have with 2000 blocks in 2000 and 2010 TIGER/Line files. NHGIS will use this info to compute lower and upper bounds.\r",
      "\r\n",
      "\r",
      "\r\n",
      "___Notes___\r",
      "\r\n",
      "This file is preliminary: at the time of its online publication, NHGIS had not yet generated and published any time series tables that include 1990 data standardized to 2010 census units, but we expect to use the weights in this crosswalk to complete that work.\r",
      "\r\n",
      "\r",
      "\r\n",
      "Complete documentation on the interpolation model used to generate the weights in this crosswalk is NOT yet available. In short, the model is based on \"cascading density weighting\", as introduced in Chapter 3 of Jonathan Schroeder's dissertation (_Visualizing Patterns in U.S. Urban Population Trends_, University of Minnesota) available here: http://hdl.handle.net/11299/48076.\r",
      "\r\n",
      "\r",
      "\r\n",
      "The general sequence of operations:\r",
      "\r\n",
      "1. Use 2010 census population and housing unit densities (summed) to guide the allocation of 2000 population and housing units among 2000-2010 block intersections.\r",
      "\r\n",
      "2. Use the estimated 2000 population and housing unit densities from step 1 to guide the allocation of 1990 characteristics (as represented by the weights in the crosswalk) among 1990-2010 block intersections.\r",
      "\r\n",
      "\r",
      "\r\n",
      "The procedure also combines two types of overlay to model intersections between 1990, 2000, and 2010 blocks:\r",
      "\r\n",
      "1. \"Direct overlay\" of 1990 & 2000 block polygons from 2000 TIGER/Line files with 2000 & 2010 block polygons from 2010 TIGER/Line files (with a preliminary step to georectify Hawaii's 2000 TIGER polygons to 2010 TIGER features in order to accommodate a systematic change in the coordinate system used to represent Hawaii features between the two TIGER versions)\r",
      "\r\n",
      "2. \"Indirect overlay\":\r",
      "\r\n",
      "\ta. Overlay 1990 & 2000 block polygons using the 2000 TIGER/Line basis\r",
      "\r\n",
      "\tb. Overlay 2000 & 2010 block polygons using the 2010 TIGER/Line basis\r",
      "\r\n",
      "\tc. Multiply 1990-2000 intersection proportions from step 2a with 2000-2010 proportions from step 2b to compute estimated proportions of each 1990 block within each 2010 block\r",
      "\r\n",
      "\r",
      "\r\n",
      "The direct overlay weights are then constrained & rescaled to eliminate any 1990-2010 intersections that are not valid in the indirect overlay. This generally prevents \"slivers\" (invalid intersections caused by changes in TIGER feature representations) from being assigned any weight.\r",
      "\r\n",
      "\r",
      "\r\n",
      "The final weighting blends weights from constrained direct overlay (CDO) and indirect overlay (IO) through a weighted average, giving high weight to CDO (and low weight to IO) in cases where most of the 1990 block's 2000 TIGER polygon has valid intersections with 2010 TIGER 2010 block polygons _and_ where the 1990-2000 block intersection comprises less than the entirety of the 2000 block. In cases where a 1990-2000 block intersection covers the entirety of the 2000 block _or_ the 1990 block's 2000 TIGER polygon has _no_ valid intersection with a 2010 TIGER 2010 block polygon, then the final weighting is based on IO alone.\r",
      "\r\n",
      "\r",
      "\r\n",
      "___Archive History___\r",
      "\r\n",
      "Author: Jonathan Schroeder\r",
      "\r\n",
      "Created: April 2017\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat capture/NHGIS_1990/NHGIS_block1990_to_block2010/readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://data2.nhgis.org/extracts/97092/46/nhgis0046_csv.zip to capture/NHGIS_1990/0046.zip\n",
      "Done, wrote 338384564 bytes to capture/NHGIS_1990/0046.zip\n",
      "Unzipping capture/NHGIS_1990/0046.zip into capture/NHGIS_1990/0046.tmp\n",
      "Success, created capture/NHGIS_1990/0046\n",
      "-rw-rw-r-- 1 rsargent rsargent 4.2G Jul 20 19:17 capture/NHGIS_1990/0046/nhgis0046_csv/nhgis0046_ds120_1990_block.csv\n",
      "capture/NHGIS_1990/0047.zip already downloaded\n",
      "capture/NHGIS_1990/0047.zip already unzipped\n",
      "-rw-rw-r-- 1 rsargent rsargent 1.8G Jul 20 16:54 capture/NHGIS_1990/0047/nhgis0047_csv/nhgis0047_ds120_1990_block.csv\n",
      "capture/NHGIS_1990/0048.zip already downloaded\n",
      "capture/NHGIS_1990/0048.zip already unzipped\n",
      "-rw-rw-r-- 1 rsargent rsargent 1.8G Jul 20 16:57 capture/NHGIS_1990/0048/nhgis0048_csv/nhgis0048_ds120_1990_block.csv\n",
      "['capture/NHGIS_1990/0046/nhgis0046_csv/nhgis0046_ds120_1990_block.csv', 'capture/NHGIS_1990/0047/nhgis0047_csv/nhgis0047_ds120_1990_block.csv', 'capture/NHGIS_1990/0048/nhgis0048_csv/nhgis0048_ds120_1990_block.csv']\n"
     ]
    }
   ],
   "source": [
    "nhgis_userid = 97092\n",
    "\n",
    "# Corrupt files:\n",
    "# 18: 11-15.  Split into 38, 39, 40\n",
    "#          38: 11, 14, 15 is good\n",
    "#          39: 12 is corrupt\n",
    "#          40: 13 is good\n",
    "# 26: 51-55.  Resubmitted as 36, corrupt.  Resubmitted as 43, corrupt\n",
    "# 33: 86-90.  Resubmitted as 42, corrupt.\n",
    "# 34: 91-95.  Resubmitted as 37, corrupt\n",
    "\n",
    "#nhgis_extract_numbers = [16, 17, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 31, 32, 35, 38, 40, 41]\n",
    "nhgis_extract_numbers = [46, 47, 48]\n",
    "#nhgis_extract_numbers = [46]\n",
    "\n",
    "decennial_datafiles = []\n",
    "\n",
    "for extract_no in nhgis_extract_numbers:\n",
    "    extract_no_dddd = '%04d' % extract_no\n",
    "    source = 'https://data2.nhgis.org/extracts/{nhgis_userid}/{extract_no}/nhgis{extract_no_dddd}_csv.zip'.format(**locals())\n",
    "    dest = '{capture90}/{extract_no_dddd}'.format(**locals())\n",
    "    download_file(source, dest + '.zip')\n",
    "    unzip_file(dest + '.zip')\n",
    "    csvfiles = glob.glob(dest + '/*/*.csv')\n",
    "    assert len(csvfiles) == 1\n",
    "    csvfile = csvfiles[0]\n",
    "    decennial_datafiles.append(csvfile)\n",
    "    !ls -lh $csvfile\n",
    "print decennial_datafiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Construct mapping from 1990 GEOID to decennial row\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def canonicalize_nhgis_1990_gisjoin(g):\n",
    "    (leader, g) = (g[0:1], g[1:])    # always G, ignore\n",
    "    assert leader == 'G'\n",
    "    (state, g) = (g[0:2], g[2:])     # state 2 digits\n",
    "    (zero, g) = (g[0:1], g[1:])      # always zero, ignore\n",
    "    assert zero == '0'\n",
    "    (county, g) = (g[0:3], g[3:])    # county 4 digits\n",
    "    (zero, g) = (g[0:1], g[1:])      # always zero, ignore\n",
    "    assert zero == '0'\n",
    "    if len(g) > 8:                   # tract is 4 or 6 digits\n",
    "        (tract, g) = (g[0:6], g[6:])  \n",
    "    else:\n",
    "        (tract, g) = (g[0:4] + '00', g[4:])\n",
    "    block = g                        # block is 3 or 4 chars\n",
    "    assert len(g) == 3 or len(g) == 4\n",
    "    ret = state + county + tract + block\n",
    "    assert len(ret) == 14 or len(ret) == 15\n",
    "    return ret\n",
    "\n",
    "#print canonicalize_nhgis_1990_gisjoin('01000100201103')\n",
    "#print canonicalize_nhgis_1990_gisjoin('01000100201101A')\n",
    "#print canonicalize_nhgis_1990_gisjoin('0100030010701122')\n",
    "#print canonicalize_nhgis_1990_gisjoin('0100030010702134A')\n",
    "\n",
    "def row_names_from_decennial_datafile(decfile):\n",
    "    for _ in stopwatch('Reading row names from %s' % decfile):\n",
    "        original_row_names = pandas.read_csv(decfile, usecols=[0], skiprows=[1], memory_map=True)\n",
    "    return [canonicalize_nhgis_1990_gisjoin(g) for g in list(original_row_names['GISJOIN'])]\n",
    "\n",
    "# print row_names_from_decennial_datafile(decennial_datafiles[0])[0:10]\n",
    "# print row_names_from_decennial_datafile(decennial_datafiles[-1])[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading row names from capture/NHGIS_1990/0046/nhgis0046_csv/nhgis0046_ds120_1990_block.csv took 62.4 seconds\n",
      "Reading row names from capture/NHGIS_1990/0047/nhgis0047_csv/nhgis0047_ds120_1990_block.csv took 13.8 seconds\n",
      "Reading row names from capture/NHGIS_1990/0048/nhgis0048_csv/nhgis0048_ds120_1990_block.csv took 13.1 seconds\n",
      "Decennial datafiles have 4934106 rows each\n"
     ]
    }
   ],
   "source": [
    "row_names_1990 = row_names_from_decennial_datafile(decennial_datafiles[0])\n",
    "\n",
    "# Confirm all 1990 decennial datafiles have same rows\n",
    "# This check already succeeded 2017 Mac\n",
    "\n",
    "confirm_same_rows = True\n",
    "\n",
    "if confirm_same_rows:\n",
    "    for decfile in decennial_datafiles[1:]:\n",
    "        assert(row_names_from_decennial_datafile(decfile) == row_names_1990)\n",
    "\n",
    "print 'Decennial datafiles have %d rows each' % len(row_names_1990)    \n",
    "\n",
    "geoid2rowidx1990 = {}\n",
    "\n",
    "for i in range(0, len(row_names_1990)):\n",
    "        geoid2rowidx1990[row_names_1990[i]] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 1990 to 2010 crosswalk\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading crosswalk took 22.1 seconds\n",
      "Crosswalk has 20286557 rows\n",
      "Crosswalk has 11078297 2010 GEOIDs\n"
     ]
    }
   ],
   "source": [
    "for _ in stopwatch('Reading crosswalk'):\n",
    "    crosswalk = pandas.read_csv(crosswalk_file, names=['geoid1990', 'geoid2010', 'weight', 'parea'], memory_map=True)\n",
    "\n",
    "print 'Crosswalk has %d rows' % len(crosswalk)\n",
    "\n",
    "sorted_crosswalk_geoids_2010 = [g[1:] for g in sorted(set(crosswalk['geoid2010']))]\n",
    "\n",
    "print 'Crosswalk has %d 2010 GEOIDs' % len(sorted_crosswalk_geoids_2010)\n",
    "\n",
    "# Number of blocks to expect in 2010\n",
    "block2010_count = 11078297\n",
    "\n",
    "assert block2010_count == len(sorted_crosswalk_geoids_2010)\n",
    "\n",
    "# But we index starting at 1 in the numpy vectors,\n",
    "# so they're length 11078298\n",
    "assert block2010_count + 1 == len(numpy.load('columncache/census2000_block2010/H0020001.numpy'))\n",
    "\n",
    "# numpy vectors are float32\n",
    "assert numpy.float32 == numpy.load('columncache/census2000_block2010/H0020001.numpy').dtype\n",
    "\n",
    "# Map 2010 blocks to vector indices\n",
    "geoid2rowidx2010 = {}\n",
    "for i in range(0, block2010_count):\n",
    "    geoid2rowidx2010[sorted_crosswalk_geoids_2010[i]] = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating crosswalk matrix took 31.0 seconds\n",
      "Populating crosswalk matrix took 137.4 seconds\n"
     ]
    }
   ],
   "source": [
    "# Construct a sparse crosswalk matrix C so that we can multiply\n",
    "# decennial 1990 data D with len(row_names_1990) rows to create\n",
    "# interpolated data I with block2010_count+1 rows \n",
    "\n",
    "for _ in stopwatch('Creating crosswalk matrix'):\n",
    "    crosswalk_matrix = scipy.sparse.lil_matrix((block2010_count + 1,len(row_names_1990)))\n",
    "    \n",
    "#for _ in stopwatch('Sleeping for half a second'):\n",
    "#    time.sleep(0.5)\n",
    "\n",
    "for _ in stopwatch('Populating crosswalk matrix'):\n",
    "    for (geoid1990, geoid2010, weight, darea) in crosswalk.itertuples(index=False, name=None):\n",
    "        geoid1990 = geoid1990[1:]\n",
    "        geoid2010 = geoid2010[1:]\n",
    "        if geoid1990 in geoid2rowidx1990:\n",
    "            r = geoid2rowidx2010[geoid2010]\n",
    "            c = geoid2rowidx1990[geoid1990]\n",
    "            crosswalk_matrix[r,c] = weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columnMap = json.load(open('capture/NHGIS_1990/columnMap.json'))\n",
    "\n",
    "def canonicalize_column_name(c):\n",
    "    prefix = 'census1990_block2010.'\n",
    "    return columnMap[prefix + c].replace(prefix, '')\n",
    "\n",
    "def interpolate_nhgis_1990_datafile(csvfilename):\n",
    "    header = csv.reader(open(csvfilename)).next()\n",
    "    first_data_col = 26\n",
    "    assert(header[first_data_col - 1] == 'ANPSADPI')\n",
    "    all_cols = range(first_data_col, len(header))\n",
    "    shard_size = 50\n",
    "    nshards = math.ceil(float(len(all_cols)) / shard_size)\n",
    "    print '%s has %d data columns, dividing into %d shards' % (csvfilename, len(all_cols), nshards)\n",
    "    \n",
    "    for shardno, cols in enumerate(numpy.array_split(all_cols, nshards)):\n",
    "        \n",
    "        for _ in stopwatch('Reading columns %d-%d (shard %d of %d) from %s' % (\n",
    "                min(cols), max(cols), shardno + 1, nshards, csvfilename)):\n",
    "            p = pandas.read_csv(csvfilename, usecols=cols, skiprows=[1], dtype=numpy.float64, memory_map=True)\n",
    "            d = p.as_matrix()\n",
    "            colnames = p.columns.values\n",
    "            \n",
    "        for _ in stopwatch('Interpolating %d columns' % len(cols)):\n",
    "            interpolated = crosswalk_matrix * d\n",
    "            \n",
    "        for _ in stopwatch('Writing %d columns into columncache' % len(cols)):\n",
    "            for i in range(0, len(colnames)):\n",
    "                canonical_colname = canonicalize_column_name(colnames[i])\n",
    "                dest = 'columncache/census1990_block2010/{canonical_colname}.numpy'.format(**locals())\n",
    "                col = interpolated[:, i].astype(numpy.float32)\n",
    "                assert len(col) == block2010_count + 1\n",
    "                assert col.dtype == numpy.float32\n",
    "                numpy_atomic_save(dest, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['capture/NHGIS_1990/0046/nhgis0046_csv/nhgis0046_ds120_1990_block.csv', 'capture/NHGIS_1990/0047/nhgis0047_csv/nhgis0047_ds120_1990_block.csv', 'capture/NHGIS_1990/0048/nhgis0048_csv/nhgis0048_ds120_1990_block.csv']\n"
     ]
    }
   ],
   "source": [
    "print decennial_datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capture/NHGIS_1990/0046/nhgis0046_csv/nhgis0046_ds120_1990_block.csv has 360 data columns, dividing into 8 shards\n",
      "Reading columns 26-70 (shard 1 of 8) from capture/NHGIS_1990/0046/nhgis0046_csv/nhgis0046_ds120_1990_block.csv took 165.0 seconds\n",
      "Interpolating 45 columns took 96.0 seconds\n",
      "Writing 45 columns into columncache took 21.3 seconds\n",
      "Reading columns 71-115 (shard 2 of 8) from capture/NHGIS_1990/0046/nhgis0046_csv/nhgis0046_ds120_1990_block.csv took 121.0 seconds\n",
      "Interpolating 45 columns took 89.5 seconds\n",
      "Writing 45 columns into columncache took 12.2 seconds\n",
      "Reading columns 116-160 (shard 3 of 8) from capture/NHGIS_1990/0046/nhgis0046_csv/nhgis0046_ds120_1990_block.csv took 74.0 seconds\n",
      "Interpolating 45 columns took 96.4 seconds\n",
      "Writing 45 columns into columncache took 8.5 seconds\n",
      "Reading columns 161-205 (shard 4 of 8) from capture/NHGIS_1990/0046/nhgis0046_csv/nhgis0046_ds120_1990_block.csv took 53.4 seconds\n",
      "Interpolating 45 columns took 26.0 seconds\n",
      "Writing 45 columns into columncache took 7.9 seconds\n",
      "Reading columns 206-250 (shard 5 of 8) from capture/NHGIS_1990/0046/nhgis0046_csv/nhgis0046_ds120_1990_block.csv took 51.4 seconds\n",
      "Interpolating 45 columns took 44.6 seconds\n",
      "Writing 45 columns into columncache took 8.0 seconds\n",
      "Reading columns 251-295 (shard 6 of 8) from capture/NHGIS_1990/0046/nhgis0046_csv/nhgis0046_ds120_1990_block.csv took 53.0 seconds\n",
      "Interpolating 45 columns took 25.3 seconds\n",
      "Writing 45 columns into columncache took 8.1 seconds\n",
      "Reading columns 296-340 (shard 7 of 8) from capture/NHGIS_1990/0046/nhgis0046_csv/nhgis0046_ds120_1990_block.csv took 50.7 seconds\n",
      "Interpolating 45 columns took 25.5 seconds\n",
      "Writing 45 columns into columncache took 8.0 seconds\n",
      "Reading columns 341-385 (shard 8 of 8) from capture/NHGIS_1990/0046/nhgis0046_csv/nhgis0046_ds120_1990_block.csv took 48.7 seconds\n",
      "Interpolating 45 columns took 21.5 seconds\n",
      "Writing 45 columns into columncache took 12.1 seconds\n",
      "capture/NHGIS_1990/0047/nhgis0047_csv/nhgis0047_ds120_1990_block.csv has 90 data columns, dividing into 2 shards\n",
      "Reading columns 26-70 (shard 1 of 2) from capture/NHGIS_1990/0047/nhgis0047_csv/nhgis0047_ds120_1990_block.csv took 29.5 seconds\n",
      "Interpolating 45 columns took 12.5 seconds\n",
      "Writing 45 columns into columncache took 13.7 seconds\n",
      "Reading columns 71-115 (shard 2 of 2) from capture/NHGIS_1990/0047/nhgis0047_csv/nhgis0047_ds120_1990_block.csv took 31.5 seconds\n",
      "Interpolating 45 columns took 24.9 seconds\n",
      "Writing 45 columns into columncache took 13.5 seconds\n",
      "capture/NHGIS_1990/0048/nhgis0048_csv/nhgis0048_ds120_1990_block.csv has 100 data columns, dividing into 2 shards\n",
      "Reading columns 26-75 (shard 1 of 2) from capture/NHGIS_1990/0048/nhgis0048_csv/nhgis0048_ds120_1990_block.csv took 27.7 seconds\n",
      "Interpolating 50 columns took 14.4 seconds\n",
      "Writing 50 columns into columncache took 13.3 seconds\n",
      "Reading columns 76-125 (shard 2 of 2) from capture/NHGIS_1990/0048/nhgis0048_csv/nhgis0048_ds120_1990_block.csv took 35.2 seconds\n",
      "Interpolating 50 columns took 31.9 seconds\n",
      "Writing 50 columns into columncache took 10.7 seconds\n"
     ]
    }
   ],
   "source": [
    "failed = []\n",
    "for file in decennial_datafiles:\n",
    "    try:\n",
    "        interpolate_nhgis_1990_datafile(file)\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        failed.append(file)\n",
    "if failed:\n",
    "    raise Exception('Some files failed to load: %s' % (' '.join(failed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
